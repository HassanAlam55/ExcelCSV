{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6834b253",
   "metadata": {},
   "source": [
    "# Code to Read CSV and Excel Fiel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dac6d9d",
   "metadata": {},
   "source": [
    "## Begin Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94d575a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eeda927a",
   "metadata": {},
   "source": [
    "## End Load Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df25a31",
   "metadata": {},
   "source": [
    "## Begin Read Excel from\n",
    "1. from sheet in excel\n",
    "2. read into dataframe\n",
    "3. Start with row that has first value of keyword in the column\n",
    "4. Save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ab42d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_excel_from_keyword_robust(\n",
    "    input_file='data.xlsx',\n",
    "    source_dir='.',\n",
    "    output_file='output',\n",
    "    output_dir='.',\n",
    "    column_name='Column1',\n",
    "    keyword='START',\n",
    "    sheet_name=0\n",
    "):\n",
    "    \"\"\"Process Excel with better error handling\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Build and check input path\n",
    "        input_path = os.path.join(source_dir, input_file)\n",
    "        if not os.path.exists(input_path):\n",
    "            print(f\"❌ File not found: {input_path}\")\n",
    "            return None\n",
    "        \n",
    "        # Read Excel\n",
    "        df = pd.read_excel(input_path, sheet_name=sheet_name)\n",
    "        \n",
    "        # Check if column exists\n",
    "        if column_name not in df.columns:\n",
    "            print(f\"❌ Column '{column_name}' not found. Available columns: {list(df.columns)}\")\n",
    "            return None\n",
    "        \n",
    "        # Find keyword\n",
    "        mask = df[column_name] == keyword\n",
    "        if not mask.any():\n",
    "            print(f\"❌ Keyword '{keyword}' not found in column '{column_name}'\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Filter from first occurrence\n",
    "        first_index = df[mask].index[0]\n",
    "        filtered_df = df.iloc[first_index:]\n",
    "        \n",
    "        # Save with date\n",
    "        date_suffix = datetime.now().strftime('%Y%m%d')\n",
    "        output_path = os.path.join(output_dir, f\"{output_file}_{date_suffix}.csv\")\n",
    "        filtered_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"✓ Success! Saved {len(filtered_df)} rows to: {output_path}\")\n",
    "        return filtered_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ebdad1",
   "metadata": {},
   "source": [
    "## End read excel and convert to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ef415f",
   "metadata": {},
   "source": [
    "## Begin Combine Excel Files\n",
    "1. read a set of files\n",
    "2. all files have date as first column\n",
    "3. output to one csv with all date\n",
    "4. output to one csve with common dates only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aacdcfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "\n",
    "def combine_excel_files(file_list, output_prefix=\"combined\"):\n",
    "    # Get current date for output file naming\n",
    "    date_stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Read all Excel files into a list of dataframes\n",
    "    dfs = []\n",
    "    for i, file in enumerate(file_list):\n",
    "        df = pd.read_excel(file)\n",
    "        # Add suffix to columns (except Date) to avoid naming conflicts\n",
    "        df.columns = ['Date'] + [f\"{col}_file{i+1}\" for col in df.columns[1:]]\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # File 1: Superset - Full outer join on Date\n",
    "    df_superset = reduce(lambda left, right: pd.merge(left, right, on='Date', how='outer'), dfs)\n",
    "    df_superset.sort_values('Date', inplace=True)\n",
    "    \n",
    "    # File 2: Common dates - Inner join on Date\n",
    "    df_common = reduce(lambda left, right: pd.merge(left, right, on='Date', how='inner'), dfs)\n",
    "    df_common.sort_values('Date', inplace=True)\n",
    "    \n",
    "    # Save to CSV files\n",
    "    df_superset.to_csv(f\"{output_prefix}_all_dates_{date_stamp}.csv\", index=False)\n",
    "    df_common.to_csv(f\"{output_prefix}_common_dates_{date_stamp}.csv\", index=False)\n",
    "    \n",
    "    print(f\"Created: {output_prefix}_all_dates_{date_stamp}.csv (rows: {len(df_superset)})\")\n",
    "    print(f\"Created: {output_prefix}_common_dates_{date_stamp}.csv (rows: {len(df_common)})\")\n",
    "\n",
    "# # Example usage\n",
    "# excel_files = ['file1.xlsx', 'file2.xlsx', 'file3.xlsx']\n",
    "# combine_excel_files(excel_files, \"merged_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c6dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "from pathlib import Path\n",
    "# Relative to notebook\n",
    "# \"C:\\Users\\Hassan\\Dropbox\\GithubRepo\\MarketPred\\DownloadedCleanedInput\\Breadth_MA200.xlsx\"\n",
    "data_folder = Path(\"../MarketPred/DownloadedCleanedInput\")\n",
    "file1 = data_folder / \"Breadth_MA200.xlsx\"\n",
    "file2 = data_folder / \"Cape.xlsx\"\n",
    "file3 = data_folder / \"PriceVolume.xlsx\"\n",
    "file4 = data_folder / \"Spread.xlsx\"\n",
    "file5 = data_folder / \"Yield_Curve.xlsx\"\n",
    "\n",
    "excel_files = [file1, file2, file3, file4, file5]\n",
    "combine_excel_files(excel_files, \"merged_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effea5e0",
   "metadata": {},
   "source": [
    "### Begin normalize date format\n",
    "1. change code below to fill na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ca4ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def merge_excel_files_normalized_dates(\n",
    "    file_paths,\n",
    "    date_column,\n",
    "    merge_type='outer',\n",
    "    output_dir='.',\n",
    "    output_prefix='merged'\n",
    "):\n",
    "    \"\"\"\n",
    "    Merge multiple Excel files on a date column with date format normalization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_paths : list\n",
    "        List of Excel file paths to merge\n",
    "    date_column : str\n",
    "        Name of the date column to merge on\n",
    "    merge_type : str\n",
    "        'outer' for superset (all dates), 'inner' for intersection (common dates only)\n",
    "    output_dir : str\n",
    "        Directory to save output file (default: current directory)\n",
    "    output_prefix : str\n",
    "        Prefix for output filename\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Merged dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    def normalize_date_column(df, date_col):\n",
    "        \"\"\"Convert date column to standard datetime format\"\"\"\n",
    "        try:\n",
    "            # Try to convert to datetime, handling multiple formats\n",
    "            df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "            # Optionally normalize to just date (remove time component)\n",
    "            df[date_col] = df[date_col].dt.date\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not normalize dates - {e}\")\n",
    "        return df\n",
    "    \n",
    "    # Validate files exist\n",
    "    for fp in file_paths:\n",
    "        if not os.path.exists(fp):\n",
    "            raise FileNotFoundError(f\"File not found: {fp}\")\n",
    "    \n",
    "    # Read all files\n",
    "    dfs = []\n",
    "    for fp in file_paths:\n",
    "        try:\n",
    "            df = pd.read_excel(fp)\n",
    "            \n",
    "            # Validate date column exists\n",
    "            if date_column not in df.columns:\n",
    "                raise ValueError(f\"Column '{date_column}' not found in {fp}\")\n",
    "            \n",
    "            # Normalize date column\n",
    "            df = normalize_date_column(df, date_column)\n",
    "            \n",
    "            # Add source file info to column names (except date column)\n",
    "            file_name = os.path.splitext(os.path.basename(fp))[0]\n",
    "            df.columns = [date_column if col == date_column else f\"{col}_{file_name}\" \n",
    "                         for col in df.columns]\n",
    "            \n",
    "            dfs.append(df)\n",
    "            print(f\"✓ Loaded and normalized: {fp}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error reading {fp}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Merge all dataframes\n",
    "    merged_df = reduce(\n",
    "        lambda left, right: pd.merge(left, right, on=date_column, how=merge_type),\n",
    "        dfs\n",
    "    )\n",
    "    \n",
    "    # Sort by date\n",
    "    merged_df = merged_df.sort_values(date_column).reset_index(drop=True)\n",
    "    \n",
    "    # Generate output filename\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_file = os.path.join(\n",
    "        output_dir, \n",
    "        f\"{output_prefix}_{merge_type}_{timestamp}.csv\"\n",
    "    )\n",
    "    \n",
    "    # Save to CSV\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\n✓ Merged file saved: {output_file}\")\n",
    "    print(f\"  Total rows: {len(merged_df)}\")\n",
    "    print(f\"  Total columns: {len(merged_df.columns)}\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# # Example usage:\n",
    "# file_paths = [\n",
    "#     'file1.xlsx',\n",
    "#     'file2.xlsx',\n",
    "#     'file3.xlsx'\n",
    "# ]\n",
    "\n",
    "# # For superset (all dates from all files)\n",
    "# df_superset = merge_excel_files_normalized_dates(\n",
    "#     file_paths=file_paths,\n",
    "#     date_column='Date',\n",
    "#     merge_type='outer',\n",
    "#     output_prefix='superset'\n",
    "# )\n",
    "\n",
    "# # For intersection (only common dates)\n",
    "# df_intersection = merge_excel_files_normalized_dates(\n",
    "#     file_paths=file_paths,\n",
    "#     date_column='Date',\n",
    "#     merge_type='inner',\n",
    "#     output_prefix='intersection'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc71542f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded and normalized: ..\\MarketPred\\DownloadedCleanedInput\\Breadth_MA200.xlsx\n",
      "✓ Loaded and normalized: ..\\MarketPred\\DownloadedCleanedInput\\Cape.xlsx\n",
      "✓ Loaded and normalized: ..\\MarketPred\\DownloadedCleanedInput\\PriceVolume.xlsx\n",
      "✓ Loaded and normalized: ..\\MarketPred\\DownloadedCleanedInput\\Spread.xlsx\n",
      "✓ Loaded and normalized: ..\\MarketPred\\DownloadedCleanedInput\\Yield_Curve.xlsx\n",
      "\n",
      "✓ Merged file saved: .\\superset_outer_20251221_090106.csv\n",
      "  Total rows: 52695\n",
      "  Total columns: 7\n",
      "✓ Loaded and normalized: ..\\MarketPred\\DownloadedCleanedInput\\Breadth_MA200.xlsx\n",
      "✓ Loaded and normalized: ..\\MarketPred\\DownloadedCleanedInput\\Cape.xlsx\n",
      "✓ Loaded and normalized: ..\\MarketPred\\DownloadedCleanedInput\\PriceVolume.xlsx\n",
      "✓ Loaded and normalized: ..\\MarketPred\\DownloadedCleanedInput\\Spread.xlsx\n",
      "✓ Loaded and normalized: ..\\MarketPred\\DownloadedCleanedInput\\Yield_Curve.xlsx\n",
      "\n",
      "✓ Merged file saved: .\\intersection_inner_20251221_090110.csv\n",
      "  Total rows: 6015\n",
      "  Total columns: 7\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "data_folder = Path(\"../MarketPred/DownloadedCleanedInput\")\n",
    "file1 = data_folder / \"Breadth_MA200.xlsx\"\n",
    "file2 = data_folder / \"Cape.xlsx\"\n",
    "file3 = data_folder / \"PriceVolume.xlsx\"\n",
    "file4 = data_folder / \"Spread.xlsx\"\n",
    "file5 = data_folder / \"Yield_Curve.xlsx\"\n",
    "\n",
    "file_paths = [file1, file2, file3, file4, file5]\n",
    "# file_paths = [\n",
    "#     'file1.xlsx',\n",
    "#     'file2.xlsx',\n",
    "#     'file3.xlsx'\n",
    "# ]\n",
    "\n",
    "# For superset (all dates from all files)\n",
    "df_superset = merge_excel_files_normalized_dates(\n",
    "    file_paths=file_paths,\n",
    "    date_column='Date',\n",
    "    merge_type='outer',\n",
    "    output_prefix='superset'\n",
    ")\n",
    "\n",
    "# For intersection (only common dates)\n",
    "df_intersection = merge_excel_files_normalized_dates(\n",
    "    file_paths=file_paths,\n",
    "    date_column='Date',\n",
    "    merge_type='inner',\n",
    "    output_prefix='intersection'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c419d0",
   "metadata": {},
   "source": [
    "### End normalize date format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4000c4b6",
   "metadata": {},
   "source": [
    "## End Combine Excel files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65e976c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csv_excel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
